# -*- coding: utf-8 -*-
"""donkey_car_dnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZQNxiEaJQk5fZb75LkUpI3hu7z2Dyniw
"""

# -*- coding: utf-8 -*-
"""donkey_car_qnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZQNxiEaJQk5fZb75LkUpI3hu7z2Dyniw
"""

# -*- coding: utf-8 -*-
"""donkey_car.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZQNxiEaJQk5fZb75LkUpI3hu7z2Dyniw
"""

import gym
import torch.optim.lr_scheduler as lr_scheduler
import argparse
import gym_donkeycar
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import pennylane as qml
from collections import deque
import random
from torch.nn.parameter import Parameter
import torch.nn.functional as F
import time


# Argument parsing setup
parser = argparse.ArgumentParser(description="Quantum Neural Network for Donkey Car")
parser.add_argument("--env_name", type=str, default="donkey-warehouse-v0", help="Donkey Car environment name")
parser.add_argument("--port", type=int, default=9091, help="Port to use for connecting to the simulator")
parser.add_argument("--episodes", type=int, default=10, help="Number of episodes to train")
parser.add_argument("--learning_rate", type=float, default=0.001, help="Learning rate for the optimizer")
parser.add_argument("--gamma", type=float, default=0.99, help="Discount factor for Q-learning")
parser.add_argument("--n_qubits", type=int, default=4, help="Number of qubits in the quantum circuit")
parser.add_argument("--save_model", type=str, default="dnn_donkeycar.pth", help="File path to save the trained model")
args = parser.parse_args()



class Actor(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(Actor, self).__init__()
        self.fc1 = nn.Linear(input_dim, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, output_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = torch.tanh(self.fc3(x))
        return x

class Critic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Critic, self).__init__()
        self.fc1 = nn.Linear(state_dim + action_dim, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 1)

    def forward(self, state, action):
        x = torch.cat([state, action], dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class DDPGAgent:
    def __init__(self, state_dim, action_dim, learning_rate=0.001, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.actor = Actor(state_dim, action_dim)  # Use the NN-based Actor
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)
        self.actor_scheduler = lr_scheduler.StepLR(self.actor_optimizer, step_size=100, gamma=0.95)
        # The Critic part remains unchanged
        self.critic = Critic(state_dim, action_dim)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate * 0.5)
        self.critic_scheduler = lr_scheduler.StepLR(self.critic_optimizer, step_size=100, gamma=0.95)
        # Exploration settings remain the same
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay

    def select_action(self, state, noise_scale=0.1):
        state = torch.as_tensor(state, dtype=torch.float32).unsqueeze(0)  # Updated based on warning
        with torch.no_grad():
            action = self.actor(state).squeeze(0).cpu().numpy()

        # Adding noise for exploration
        noise = noise_scale * np.random.randn(self.action_dim)
        action += noise

        action[0,1] = np.clip(np.abs(action[0,1]), 0.1, 1.0)  # Ensure car doesn't drive backward
        action = action.flatten()
        return np.clip(action, -1, 1)

    # Update function to be called every time step
    def update(self, replay_buffer, batch_size=64, gamma=0.99):
        # Sample a batch of experiences from the replay buffer
        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)

        states = torch.FloatTensor(states)
        actions = torch.FloatTensor(actions)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones).unsqueeze(1)

        # Compute the target Q value
        next_actions = self.actor(next_states)
        next_Q_values = self.critic(next_states, next_actions.detach())
        Q_targets = rewards + (gamma * next_Q_values * (1 - dones))
        # Compute current Q values
        Q_expected = self.critic(states, actions)

        # Critic loss
        critic_loss = F.mse_loss(Q_expected, Q_targets.detach())

        # Update critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # Actor loss
        actor_loss = -self.critic(states, self.actor(states)).mean()

        # Update actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # Update exploration noise
        self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)

def preprocess_state(state):
    state = state / 255.0  # Normalize the input state
    state = np.transpose(state, (2, 0, 1))  # Adjust dimensions if necessary
    state = state.flatten()  # Flatten the state to create a single long vector
    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Ensure dtype is torch.float32
    return state_tensor





def train(env, agent, episodes, save_path):
    start_time = time.time()
    total_rewards = []
    for episode in range(episodes):
        state = env.reset()
        state = preprocess_state(state)
        episode_reward = 0
        done = False

        while not done:
            action = agent.select_action(state)
            next_state, reward, done, _ = env.step(action)
            next_state = preprocess_state(next_state)

            # Assuming you have a function to store transitions in replay buffer here

            state = next_state
            episode_reward += reward

        total_rewards.append(episode_reward)
        print(f"Episode: {episode}, Total Reward: {episode_reward}")

        # Perform learning update here
        # Assuming you have a function to perform learning update here

        # Decay epsilon and update learning rate
        agent.epsilon = max(agent.epsilon_min, agent.epsilon_decay * agent.epsilon)
        agent.actor_scheduler.step()
        agent.critic_scheduler.step()

        if episode % 10 == 0:
            torch.save(agent.actor.state_dict(), save_path)

    # Output metrics
    average_reward = sum(total_rewards) / episodes
    print(f"Average Reward: {average_reward}")
    print(f"Training Time: {time.time() - start_time}s")


if __name__ == "__main__":
    env = gym.make(args.env_name, conf={"port": args.port})
    state_dim = np.product(env.observation_space.shape)
    action_dim = env.action_space.shape[0]

    agent = DDPGAgent(state_dim=state_dim, action_dim=action_dim, learning_rate=args.learning_rate)
    train(env, agent, args.episodes, args.save_model)

